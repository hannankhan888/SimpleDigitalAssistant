Index: VoiceRecognition/Wav2vecLive/realTimeAudio.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pyaudio\r\nimport webrtcvad\r\nfrom .inference import Wave2Vec2Inference\r\nimport numpy as np\r\nimport threading\r\nimport time\r\nfrom sys import exit\r\nimport contextvars\r\nfrom queue import  Queue\r\nimport os\r\n\r\n\r\nclass LiveWav2Vec2:\r\n    exit_event = threading.Event()    \r\n    def __init__(self, model_name, lm_path=None, device_name=\"default\"):\r\n        self.model_name = model_name\r\n        self.device_name = device_name  \r\n        self.lm_path = lm_path            \r\n\r\n    def stop(self):\r\n        \"\"\"stop the asr process\"\"\"\r\n        LiveWav2Vec2.exit_event.set()\r\n        self.asr_input_queue.put(\"close\")\r\n        print(\"asr stopped\")\r\n\r\n    def start(self):\r\n        \"\"\"start the asr process\"\"\"\r\n        self.wav2vec2_initialized = False\r\n        self.asr_output_queue = Queue()\r\n        self.asr_input_queue = Queue()\r\n        self.asr_process = threading.Thread(target=LiveWav2Vec2.asr_process, args=(\r\n            self, self.model_name, self.asr_input_queue, self.asr_output_queue,))\r\n        self.asr_process.start()\r\n        # time.sleep(5)  # start vad after asr model is loaded\r\n        while not self.wav2vec2_initialized:\r\n            pass\r\n\r\n        if self.wav2vec2_initialized:\r\n            print(\"starting VAD\")\r\n            self.start_time = time.time()\r\n            self.vad_process = threading.Thread(target=LiveWav2Vec2.vad_process, args=(\r\n                self, self.device_name, self.asr_input_queue,))\r\n            self.vad_process.start()\r\n\r\n    def vad_process(self, device_name, asr_input_queue):\r\n        vad = webrtcvad.Vad()\r\n        vad.set_mode(1)\r\n\r\n        audio = pyaudio.PyAudio()\r\n        FORMAT = pyaudio.paInt16\r\n        CHANNELS = 1\r\n        RATE = 16000\r\n        # A frame must be either 10, 20, or 30 ms in duration for webrtcvad\r\n        FRAME_DURATION = 30\r\n        CHUNK = int(RATE * FRAME_DURATION / 1000)\r\n        RECORD_SECONDS = 50\r\n\r\n        microphones = LiveWav2Vec2.list_microphones(audio)\r\n        selected_input_device_id = LiveWav2Vec2.get_input_device_id(\r\n            device_name, microphones)\r\n\r\n        stream = audio.open(input_device_index=selected_input_device_id,\r\n                            format=FORMAT,\r\n                            channels=CHANNELS,\r\n                            rate=RATE,\r\n                            input=True,\r\n                            frames_per_buffer=CHUNK)\r\n\r\n        frames = b'' \r\n        frames_tag = [] # holds boolean of whether speech was detected\r\n        end_frame = 0  \r\n        exit_time = None\r\n        while True:         \r\n            if LiveWav2Vec2.exit_event.is_set():\r\n                break            \r\n            frame = stream.read(CHUNK)\r\n            frames_tag.append(vad.is_speech(frame, RATE))\r\n            \r\n            # add frames only if voice is detected\r\n            if frames_tag[-1]:\r\n                frames += frame\r\n\r\n            # add every 0.5 seconds of frames to queue\r\n            frame_diff = len(frames) - end_frame\r\n            if frame_diff > 8000:\r\n                asr_input_queue.put(frames)\r\n                end_frame = len(frames)\r\n\r\n            # if time > 2 seconds and 90% of last 1 second of frames has no voice detected then listen for 2 more seconds then exit\r\n            NUM_FRAMES = 100\r\n            percent_silence = len([ele for ele in frames_tag[-NUM_FRAMES:] if ele == False]) / NUM_FRAMES\r\n            elapsed_time = time.time() - self.start_time\r\n            if elapsed_time > 2 and (percent_silence > 0.8 and len(frames_tag[-NUM_FRAMES:]) == NUM_FRAMES):\r\n                if exit_time is None:\r\n                    exit_time = time.time()\r\n                elif time.time() - exit_time > 2:\r\n                    break\r\n                else:\r\n                    pass\r\n\r\n        stream.stop_stream()\r\n        stream.close()\r\n        audio.terminate()\r\n        self.stop()\r\n\r\n\r\n    def asr_process(self, model_name, in_queue, output_queue):\r\n        wave2vec_asr = Wave2Vec2Inference(model_name, self.lm_path)\r\n        self.wav2vec2_initialized = True\r\n\r\n        print(\"\\nlistening to your voice\\n\")\r\n        while True:                        \r\n            audio_frames = in_queue.get()       \r\n            if audio_frames == \"close\":\r\n                break\r\n\r\n            float64_buffer = np.frombuffer(\r\n                audio_frames, dtype=np.int16) / 32767\r\n            start = time.perf_counter()\r\n            text = wave2vec_asr.buffer_to_text(float64_buffer).lower()\r\n            inference_time = time.perf_counter()-start\r\n            sample_length = len(float64_buffer) / 16000  # length in sec\r\n            if text != \"\":\r\n                output_queue.put([text,sample_length,inference_time])                            \r\n\r\n    def get_input_device_id(device_name, microphones):\r\n        for device in microphones:\r\n            if device_name in device[1]:\r\n                return device[0]\r\n\r\n    def list_microphones(pyaudio_instance):\r\n        info = pyaudio_instance.get_host_api_info_by_index(0)\r\n        numdevices = info.get('deviceCount')\r\n\r\n        result = []\r\n        for i in range(0, numdevices):\r\n            if (pyaudio_instance.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\r\n                name = pyaudio_instance.get_device_info_by_host_api_device_index(\r\n                    0, i).get('name')\r\n                result += [[i, name]]\r\n        return result\r\n\r\n    def get_last_text(self):\r\n        \"\"\"returns the text, sample length and inference time in seconds.\"\"\"\r\n        return self.asr_output_queue.get()           \r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Live ASR\")\r\n    MODELS = {\r\n        \"large\": \"facebook/wav2vec2-large-960h\",\r\n        \"base\": \"facebook/wav2vec2-base-960h\",\r\n        \"distil\": \"OthmaneJ/distil-wav2vec2\"\r\n    }\r\n    LM = \"VoiceRecognition/4gram_big.arpa\"\r\n\r\n    asr = LiveWav2Vec2(model_name=MODELS[\"distil\"], lm_path=LM)\r\n    asr.start()\r\n\r\n    try:        \r\n        while True:\r\n            text,sample_length,inference_time = asr.get_last_text()                        \r\n            print(f\"{sample_length:.3f}s\\t{inference_time:.3f}s\\t{text}\")\r\n            \r\n    except KeyboardInterrupt:\r\n        asr.stop()  \r\n        exit()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/VoiceRecognition/Wav2vecLive/realTimeAudio.py b/VoiceRecognition/Wav2vecLive/realTimeAudio.py
--- a/VoiceRecognition/Wav2vecLive/realTimeAudio.py	(revision 9e787737d338c2cded90b5e551a194ffc68cee27)
+++ b/VoiceRecognition/Wav2vecLive/realTimeAudio.py	(date 1640643431183)
@@ -1,6 +1,6 @@
 import pyaudio
 import webrtcvad
-from .inference import Wave2Vec2Inference
+from inference import Wave2Vec2Inference
 import numpy as np
 import threading
 import time
