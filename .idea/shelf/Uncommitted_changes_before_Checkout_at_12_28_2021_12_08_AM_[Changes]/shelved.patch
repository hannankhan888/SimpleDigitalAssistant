Index: VoiceRecognition/Wav2vecLive/inference.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\nimport soundfile as sf\r\nimport torch\r\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\r\nfrom pyctcdecode import build_ctcdecoder\r\nfrom tqdm import tqdm\r\n\r\n# for testing\r\nfrom datasets import load_dataset, load_metric\r\nimport re\r\n\r\nclass Wave2Vec2Inference():\r\n    def __init__(self, model_name, lm_path=None):\r\n        try:\r\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n        except:\r\n            self.device = torch.device(\"cpu\")\r\n        self.model = (Wav2Vec2ForCTC.from_pretrained(model_name)).to(self.device)\r\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\r\n        vocab_dict = self.processor.tokenizer.get_vocab()\r\n        sorted_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\r\n        if lm_path:\r\n            alpha=0\r\n            beta=0\r\n            beam_width = 1024\r\n            self.decoder = build_ctcdecoder(list(sorted_dict.keys()), lm_path)\r\n        else:\r\n            self.decoder = None\r\n\r\n\r\n    def buffer_to_text(self, audio_buffer):\r\n        if(len(audio_buffer)==0):\r\n            return \"\"\r\n\r\n        inputs = self.processor(audio_buffer, sampling_rate=16000, return_tensors=\"pt\").to(self.device)\r\n\r\n        if self.decoder:\r\n            with torch.no_grad():\r\n                logits = self.model(inputs.input_values).logits.numpy()[0]\r\n            transcription = self.decoder.decode(logits)\r\n        else:\r\n            with torch.no_grad():\r\n                logits = self.model(inputs.input_values).logits\r\n            predicted_ids = torch.argmax(logits, dim=-1)\r\n            transcription = self.processor.batch_decode(predicted_ids)[0]\r\n\r\n        return transcription.lower()\r\n\r\n    def file_to_text(self,filename):\r\n        audio_input, samplerate = sf.read(filename)\r\n        assert samplerate == 16000\r\n        return self.buffer_to_text(audio_input)\r\n\r\nclass TestWav2Vec2(Wave2Vec2Inference):\r\n    def __init__(self, model_name, lm_path=None):\r\n        super().__init__(model_name, lm_path=lm_path) \r\n        self.TEST_SIZE = 100   \r\n        self.model_name = model_name\r\n\r\n    def compute_wer(self):\r\n        timit = load_dataset(\"timit_asr\")\r\n        timit = timit.map(self.remove_special_characters)\r\n        timit_test = {\r\n            \"text\": [item for item in timit[\"test\"][\"text\"][0:self.TEST_SIZE]],\r\n            \"audio\": [item[\"array\"] for item in timit[\"test\"][\"audio\"][0:self.TEST_SIZE]]\r\n        }\r\n\r\n        print(\"Getting predictions...\")\r\n        results = map(self.buffer_to_text, tqdm(timit_test[\"audio\"]))\r\n        wer_metric = load_metric(\"wer\")\r\n        print(f\"Test WER for {self.model_name}: {wer_metric.compute(predictions=results, references=timit_test['text']):.3f}\")\r\n\r\n        return {\"og\": timit_test[\"text\"], \"pred\": results}\r\n\r\n    def remove_special_characters(self, batch):\r\n        chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\r\n        batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\r\n        return batch\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Model test\")\r\n    MODELS = {\r\n        \"large\": \"facebook/wav2vec2-large-960h\",\r\n        \"base\": \"facebook/wav2vec2-base-960h\",\r\n        \"distil\": \"OthmaneJ/distil-wav2vec2\"\r\n    }\r\n    LM = \"VoiceRecognition/4gram_big.arpa\"\r\n    start = time.time()\r\n    asr = Wave2Vec2Inference(model_name=MODELS[\"distil\"],lm_path=LM)\r\n    print(f\"time to initialize obect was {time.time()-start}\")\r\n    text = asr.file_to_text(\"resources/augmented_audio_files/noisy_harvard.wav\")\r\n    print(text)\r\n    \r\n    # distil_no_lm = TestWav2Vec2(model_name=MODELS[\"distil\"],lm_path=None) # WER = 0.268 on 100 samples\r\n    # distil_no_lm.compute_wer()\r\n    # distil_with_lm = TestWav2Vec2(model_name=MODELS[\"distil\"],lm_path=LM) # WER = 0.152 on 100 samples\r\n    # distil_with_lm.compute_wer()\r\n\r\n    # base_no_lm = TestWav2Vec2(model_name=MODELS[\"base\"],lm_path=None) # WER = 0.120 on 100 samples\r\n    # base_no_lm.compute_wer()\r\n    # base_with_lm = TestWav2Vec2(model_name=MODELS[\"base\"],lm_path=LM) # WER = 0.081 on 100 samples\r\n    # base_with_lm.compute_wer()\r\n\r\n    # large_no_lm = TestWav2Vec2(model_name=MODELS[\"large\"],lm_path=None) # WER = 0.101 on 100 samples\r\n    # large_no_lm.compute_wer()\r\n    # large_with_lm = TestWav2Vec2(model_name=MODELS[\"large\"],lm_path=LM) # WER = 0.068 on 100 samples\r\n    # large_with_lm.compute_wer()
===================================================================
diff --git a/VoiceRecognition/Wav2vecLive/inference.py b/VoiceRecognition/Wav2vecLive/inference.py
--- a/VoiceRecognition/Wav2vecLive/inference.py	
+++ b/VoiceRecognition/Wav2vecLive/inference.py	
@@ -86,7 +86,7 @@
     }
     LM = "VoiceRecognition/4gram_big.arpa"
     start = time.time()
-    asr = Wave2Vec2Inference(model_name=MODELS["distil"],lm_path=LM)
+    asr = Wave2Vec2Inference(model_name=MODELS["distil"],lm_path=None)
     print(f"time to initialize obect was {time.time()-start}")
     text = asr.file_to_text("resources/augmented_audio_files/noisy_harvard.wav")
     print(text)
